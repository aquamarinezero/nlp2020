{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import jieba\n",
    "import re\n",
    "import numpy as np\n",
    "from zhon.hanzi import punctuation #去除中文标点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一步，构建语料库\n",
    "选择除第一部小说之外的文本材料作为语料库来源，第一部作品作为测试文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文本替换\n",
    "def reform(text):\n",
    "    #首先去掉特殊符号以及数字字母\n",
    "    text = re.sub( '[a-zA-Z0-9’!\"#$%&\\'()*+,-./:;<=>?@?★、…【】《》“”‘’[\\\\]^_`{|}~\\s]+', \"\", text)\n",
    "    #如果是以“。”结束的则将“。”删掉\n",
    "    if (text.endswith(\"。\") or text.endswith(\"？\") or text.endswith(\"！\")):\n",
    "        text=text[:-1]\n",
    "    #添加起始符BOS和终止符EOS   \n",
    "    text1=text.replace(\"。\", \"EOSBOS\").replace(\"!\",\"EOSBOS\").replace(\"，\",\"EOSBOS\").replace(\"？\",\"EOSBOS\")\n",
    "    text2=\"BOS\"+text1+\"EOS\"\n",
    "    return text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Reddish\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在读取：碧血剑\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.691 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在读取：飞狐外传\n",
      "正在读取：连城诀\n",
      "正在读取：鹿鼎记\n",
      "正在读取：三十三剑客图\n",
      "正在读取：射雕英雄传\n",
      "正在读取：神雕侠侣\n",
      "正在读取：书剑恩仇录\n",
      "正在读取：天龙八部\n",
      "正在读取：侠客行\n",
      "正在读取：笑傲江湖\n",
      "正在读取：雪山飞狐\n",
      "正在读取：倚天屠龙记\n",
      "正在读取：鸳鸯刀\n",
      "正在读取：越女剑\n"
     ]
    }
   ],
   "source": [
    "# 首先读取文本，去掉标点符号进行分割\n",
    "word_lst = []\n",
    "key_list = []\n",
    "word_dict = {}\n",
    "\n",
    "filepath = r'../..//NLP2020/dataset/inf.txt'\n",
    "with open(filepath, 'r', encoding='utf-8') as f:\n",
    "    index = f.read()\n",
    "    index = index.split(\",\")\n",
    "    for book in index[1:]:\n",
    "        bookpath = r\"../../NLP2020/dataset/\"+book+\".txt\"\n",
    "        with open(bookpath, 'r', encoding='utf-8') as f:\n",
    "            print(\"正在读取：\"+book)\n",
    "            text = f.read()\n",
    "            text = reform(text)\n",
    "            jieba.suggest_freq(\"BOS\", True)\n",
    "            jieba.suggest_freq(\"EOS\", True)\n",
    "            text = jieba.cut(text)\n",
    "            for t in text:\n",
    "                word_lst.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计一元词频\n",
    "for item in word_lst:\n",
    "    if item not in word_dict:\n",
    "        word_dict[item] = 1\n",
    "    else:\n",
    "        word_dict[item] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计二元词频\n",
    "word_dict_bi={}\n",
    "def bilist(list_s):\n",
    "    for i in range(0,len(list_s)-1):\n",
    "        if (list_s[i+1],list_s[i]) in word_dict_bi:\n",
    "            word_dict_bi[(list_s[i+1],list_s[i])] +=1\n",
    "        else:\n",
    "            word_dict_bi[(list_s[i+1],list_s[i])] = 1\n",
    "    return  word_dict_bi\n",
    "\n",
    "word_dict_bi = bilist(word_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计三元词频\n",
    "word_dict_tri={}\n",
    "def trilist(list_s):\n",
    "    for i in range(0,len(list_s)-2):\n",
    "        if (list_s[i+2],list_s[i+1],list_s[i]) in word_dict_tri:\n",
    "            word_dict_tri[(list_s[i+2],list_s[i+1],list_s[i])] +=1\n",
    "        else:\n",
    "            word_dict_tri[(list_s[i+2],list_s[i+1],list_s[i])] =1\n",
    "    return  word_dict_tri\n",
    "\n",
    "word_dict_tri = trilist(word_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二步，读取测试文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在读取：白马啸西风\n"
     ]
    }
   ],
   "source": [
    "# 首先读取文本，去掉标点符号进行分割\n",
    "word_lst_t = []\n",
    "key_list_t = []\n",
    "word_dict_t = {}\n",
    "\n",
    "filepath = r'../..//NLP2020/dataset/inf.txt'\n",
    "with open(filepath, 'r', encoding='utf-8') as f:\n",
    "    index = f.read()\n",
    "    index = index.split(\",\")\n",
    "    book = index[0]\n",
    "    bookpath = r\"../../NLP2020/dataset/\"+book+\".txt\"\n",
    "    with open(bookpath, 'r', encoding='utf-8') as f:\n",
    "        print(\"正在读取：\"+book)\n",
    "        text = f.read()\n",
    "        text = reform(text)\n",
    "        jieba.suggest_freq(\"BOS\", True)\n",
    "        jieba.suggest_freq(\"EOS\", True)\n",
    "        text = jieba.cut(text)\n",
    "        for t in text:\n",
    "            word_lst_t.append(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第三步，计算交叉熵\n",
    "$$H(X,M) = H(X)+KL(p||m)=-\\sum_{x \\to X}p(x)log(m(x))=E_p(-logm(x))=-\\frac{1}{n}log m(x_1x_2...x_n)$$\n",
    "$$m(x_1x_2...x_n)=m(x_1x_2)\\prod_{i=3}^{n}m(x_i|x_{i-1}x_{i-2})=m(x_1x_2)m(x_3|x_1x_2)...m(x_n|x_{n-1}x_{n-2})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "计算可得交叉熵为 11.271312096091158 bit.\n"
     ]
    }
   ],
   "source": [
    "n = len(word_lst_t)\n",
    "if (word_lst_t[1],word_lst_t[0]) in word_dict_bi:\n",
    "    counts_bi= list(word_dict_bi.values())\n",
    "    counts_bi= np.array(counts_bi).astype(float)\n",
    "    sum_value =sum(word_dict_bi.values());\n",
    "    jointp = np.log2(word_dict_bi[(word_lst_t[1],word_lst_t[0])] * 1.0 / sum_value)\n",
    "else: \n",
    "    jointp = np.log2(0.000001);\n",
    "\n",
    "for i in range(2,len(word_lst_t)):\n",
    "    if ((word_lst_t[i],word_lst_t[i-1],word_lst_t[i-2]) not in word_dict_tri)or((word_lst_t[i-1],word_lst_t[i-2]) not in word_dict_bi):\n",
    "        jointp += np.log2(0.000001)\n",
    "    else:\n",
    "        count_up = word_dict_tri[(word_lst_t[i],word_lst_t[i-1],word_lst_t[i-2])]\n",
    "        count_down = word_dict_bi[(word_lst_t[i-1],word_lst_t[i-2])]\n",
    "        jointp += np.log2((count_up * 1.0 / count_down))\n",
    "cross_entropy = -jointp / n;\n",
    "print(\"计算可得交叉熵为 \"+str(cross_entropy)+\" bit.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文本替换\n",
    "def reform(text):\n",
    "    #首先去掉特殊符号以及数字字母\n",
    "    text = re.sub( '[a-zA-Z0-9’!\"#$%&\\'()*+,-./:;<=>?@?★、…【】《》“”‘’[\\\\]^_`{|}~\\s]+', \"\", text)\n",
    "    #如果是以“。”结束的则将“。”删掉\n",
    "    if (text.endswith(\"。\") or text.endswith(\"？\") or text.endswith(\"！\")):\n",
    "        text=text[:-1]\n",
    "    #添加起始符BOS和终止符EOS   \n",
    "    text1=text.replace(\"。\", \"EB\").replace(\"!\",\"EB\").replace(\"，\",\"EB\").replace(\"？\",\"EB\")\n",
    "    text2=\"B\"+text1+\"E\"\n",
    "    return text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在读取：白马啸西风\n"
     ]
    }
   ],
   "source": [
    "# 首先读取文本，去掉标点符号进行分割\n",
    "word_lst_t = []\n",
    "key_list_t = []\n",
    "word_dict_t = {}\n",
    "\n",
    "filepath = r'../..//NLP2020/dataset/inf.txt'\n",
    "with open(filepath, 'r', encoding='utf-8') as f:\n",
    "    index = f.read()\n",
    "    index = index.split(\",\")\n",
    "    book = index[0]\n",
    "    bookpath = r\"../../NLP2020/dataset/\"+book+\".txt\"\n",
    "    with open(bookpath, 'r', encoding='utf-8') as f:\n",
    "        print(\"正在读取：\"+book)\n",
    "        text = f.read()\n",
    "        text = reform(text)\n",
    "        for t in text:\n",
    "            word_lst_t.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4312860668000855"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#平均词长\n",
    "n2 = len(word_lst_t)\n",
    "word_len = n2/n\n",
    "word_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.874954111228329"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#字符交叉熵\n",
    "cross_entropy/word_len"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
